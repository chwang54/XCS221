% This contents of this file will be inserted into the _Solutions version of the
% output tex document.  Here's an example:

% If assignment with subquestion (1.a) requires a written response, you will
% find the following flag within this document: <SCPD_SUBMISSION_TAG>_1a
% In this example, you would insert the LaTeX for your solution to (1.a) between
% the <SCPD_SUBMISSION_TAG>_1a flags.  If you also constrain your answer between the
% START_CODE_HERE and END_CODE_HERE flags, your LaTeX will be styled as a
% solution within the final document.

% Please do not use the '<SCPD_SUBMISSION_TAG>' character anywhere within your code.  As expected,
% that will confuse the regular expressions we use to identify your solution.

\def\assignmentnum{3 }
\def\assignmentname{Peeking Blackjack}
\def\assignmenttitle{XCS221 Assignment \assignmentnum --- \assignmentname}
\input{macros}
\begin{document}
\pagestyle{myheadings} \markboth{}{\assignmenttitle}

% <SCPD_SUBMISSION_TAG>_entire_submission

This handout includes space for every question that requires a written response.
Please feel free to use it to handwrite your solutions (legibly, please).  If
you choose to typeset your solutions, the |README.md| for this assignment includes
instructions to regenerate this handout with your typeset \LaTeX{} solutions.
\ruleskip

\LARGE
1.a
\normalsize

% <SCPD_SUBMISSION_TAG>_1a
\begin{answer}
  
  \section*{Initialization (Iteration 0)}
  
  \begin{itemize}
      \item $V(-2) = 0$ (Terminal State)
      \item $V(-1) = 0$
      \item $V(0) = 0$
      \item $V(1) = 0$
      \item $V(2) = 0$ (Terminal State)
  \end{itemize}
  
  \section*{Iteration 1}
  
  Using the Bellman equation for value iteration, $V(s)$ values are calculated as:
  
  For state $-1$:
  \[
  V(-1) = \max\left(0.2 \times (-5 + 0) + 0.8 \times (20 + 0), 0.3 \times (-5 + 0) + 0.7 \times (20 + 0)\right)
  \]
  \[
  V(-1) = \max(16, 13.5)
  \]
  \[
  V(-1) = 16
  \]
  
  For state $0$:
  \[
  V(0) = \max\left(0.2 \times (-5 + 0) + 0.8 \times (-5 + 0), 0.3 \times (-5 + 0) + 0.7 \times (-5 + 0)\right)
  \]
  \[
  V(0) = \max(-5, -5)
  \]
  \[
  V(0) = -5
  \]
  
  For state $1$:
  \[
  V(1) = \max\left(0.2 \times (100 + 0) + 0.8 \times (-5 + 0), 0.3 \times (100 + 0) + 0.7 \times (-5 + 0)\right)
  \]
  \[
  V(1) = \max(16, 26.5)
  \]
  \[
  V(1) = 26.5
  \]
  
  \section*{Iteration 2}
  
  For state $-1$:
  \[
  V(-1) = \max\left(0.2 \times (-5 + -5) + 0.8 \times (20 + 0), 0.3 \times (-5 + -5) + 0.7 \times (20 + 0)\right)
  \]
  \[
  V(-1) = \max(14, 11)
  \]
  \[
  V(-1) = 14
  \]
  
  For state $0$:
  \[
  V(0) = \max\left(0.2 \times (-5 + 26.5) + 0.8 \times (-5 + 16), 0.3 \times (-5 + 26.5) + 0.7 \times (-5 + 16)\right)
  \]
  \[
  V(0) = \max(13.1, 14.15)
  \]
  \[
  V(0) = 14.15
  \]
  
  For state $1$:
  \[
  V(1) = \max\left(0.2 \times (100 + 0) + 0.8 \times (-5 + -5), 0.3 \times (100 + 0) + 0.7 \times (-5 + -5)\right)
  \]
  \[
  V(1) = \max(12, 23)
  \]
  \[
  V(1) = 23
  \]
  
  \section*{Summary}
  
  \textbf{After Iteration 0:}
  
  \begin{itemize}
      \item $V(-2) = 0$
      \item $V(-1) = 0$
      \item $V(0) = 0$
      \item $V(1) = 0$
      \item $V(2) = 0$
  \end{itemize}
  
  \textbf{After Iteration 1:}
  
  \begin{itemize}
      \item $V(-2) = 0$
      \item $V(-1) = 16$
      \item $V(0) = -5$
      \item $V(1) = 26.5$
      \item $V(2) = 0$
  \end{itemize}
  
  \textbf{After Iteration 2:}
  
  \begin{itemize}
      \item $V(-2) = 0$
      \item $V(-1) = 14$
      \item $V(0) = 14.15$
      \item $V(1) = 23$
      \item $V(2) = 0$
  \end{itemize}
  
  
\end{answer}
% <SCPD_SUBMISSION_TAG>_1a
\clearpage

\LARGE
1.b
\normalsize

% <SCPD_SUBMISSION_TAG>_1b
\begin{answer}
  % ### START CODE HERE ###
  \begin{itemize}
    \item $S(-1)$: the best policy is take $A(-1)$, which will have $V_{\text{opt}}(-1) = 14$
    \item $S(0)$: the best policy is take $A(1)$, which will have $V_{\text{opt}}(0) = 14.15$
    \item $S(1)$: the best policy is take $A(1)$, which will have $V_{\text{opt}}(0) = 23$
\end{itemize}
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_1b
\clearpage

\LARGE
2.a
\normalsize

% <SCPD_SUBMISSION_TAG>_2a
\begin{answer}
  % ### START CODE HERE ###
  Extend the state space by adding an artificial terminal state S(term)

  Redifine the transition actions
  \begin{itemize}
    \item for the artificial state S(term), define its transition probabilities to be $1-\lambda$ 
    \item for the original states, update its transition probabilities $T'(s,a,s')=\lambda\times T(s,a,s')$
  \end{itemize}

  Redifine the rewards
  \begin{itemize}
    \item for the artificial state S(term), define its rewards $0$ 
    \item for the original states, keep its rewards as original rewards
  \end{itemize}
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_2a
\clearpage

\LARGE
4.b
\normalsize

% <SCPD_SUBMISSION_TAG>_4b
\begin{answer}
  % ### START CODE HERE ###
  Comparing Q-learning and Value Iteration for smallMDP:

  \begin{itemize}
    \item With state (1, 1, (1, 2)): Differing actions between VI (Take) and QL (Quit)
    \item With state (5, 1, (2, 1)): Differing actions between VI (Take) and QL (Quit)
    \item With state (6, 0, (1, 1)): Differing actions between VI (Take) and QL (Quit)
  \end{itemize}

  Differing actions between VI and QL: 3


  Comparing Q-learning and Value Iteration for largeMDP:

  Differing actions between VI and QL: 880
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_4b
\clearpage

\LARGE
4.d
\normalsize

% <SCPD_SUBMISSION_TAG>_4d
\begin{answer}
  % ### START CODE HERE ###
  Comparing Q-learning and Value Iteration for newThresholdMDP:
  ValueIteration: 5 iterations
  The expected reward from simulating the original policy on the newThresholdMDP is: 6.868
  The expected reward under the new Q-learning policy is: 12.0
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_4d
\clearpage

\LARGE
5.a
\normalsize

% <SCPD_SUBMISSION_TAG>_5a
\begin{answer}
  % ### START CODE HERE ###
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_5a
\clearpage

\LARGE
5.b
\normalsize

% <SCPD_SUBMISSION_TAG>_5b
\begin{answer}
  % ### START CODE HERE ###
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_5b
\clearpage


\LARGE
5.c
\normalsize

% <SCPD_SUBMISSION_TAG>_5c
\begin{answer}
  % ### START CODE HERE ###
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_5c
\clearpage


\LARGE
5.d
\normalsize

% <SCPD_SUBMISSION_TAG>_5d
\begin{answer}
  % ### START CODE HERE ###
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_5d

\clearpage
% <SCPD_SUBMISSION_TAG>_entire_submission

\end{document}